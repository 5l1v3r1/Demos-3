<html>
<head>
	<title>Web Audio Tuner</title>
	<meta name="og:title" content="Web Audio tuner"/>
	<meta name="description" content="Chromatic tuner implemented using Web Audio and Media Stream APIs"/>
	<meta name="keywords" content="Media, web audio"/>
	<meta name="author" content="Joaquin Bonet"/>
	<link rel="stylesheet" type="text/css" href="styles/demotemplate.css"/>
	<link rel="stylesheet" type="text/css" href="styles/demo.css"/>
</head>

<body>
<article class="demo-wrap">

	<!-- DEMO INTRO -->
	<header class="demo-header">
		<div class="container">
			<heading class="section-header">
				<h1 class="heading">Web Audio tuner</h1>
			</heading>
			<div class="row">
				<div class="col-sm-12">
					<p class="text-title">This is a simple tuner built using the WebAudio API. Like tuners in the real world, you can also
					use it to generate a reference tone so you can tune by ear.</p>
				</div>
			</div>
		</div>
	</header>

	<!-- DEMO CONTENT -->
	<div class="demo-content">
		<div class="container">

			<!-- REPEATABLE SECTIONS -->
			<section class="section">
				<header class="section-header">
					<h2 class="subheading">Tuner</h2>
				</header>
				<div class="section-body">
                    <p class="msg-error" id="errorMessage"></p>
					<div class="row">
						<div class="col-sm-8">
							<div class="row">
                                <div class="pitch"><span id="pitch">-- Hz</span></div>
                                <div class="note"><span id="note">--</span></div>
                            </div>
                            <div class="row">
                                
                                <div id="cents"></div>
                                <div class="gaugeContainer">
                                    <canvas id="gaugeCanvas"></canvas>
                                </div>
                            </div>
						</div>
						<div class="col-sm-6">
							<div class="row">
                                <button class="button" id="micButton">Microphone</button>
                                <button class="button" id="refButton">Reference tone</button>
                            </div>
                            <div class="row">
                                <div id="microphoneOptions" class="tunerOptions">
                                    <fieldset>
                                        <legend>Base frequency</legend>
                                        <button class="button minusFreq">-</button>
                                        <button class="button plusFreq">+</button>
                                    </fieldset>
                                </div>
                                <div id="referenceOptions" class="tunerOptions">
                                    <fieldset>
                                        <legend>Base frequency</legend>
                                        <button class="button minusFreq">-</button>
                                        <button class="button plusFreq">+</button>
                                    </fieldset>
                                    <fieldset>
                                        <legend>Note</legend>
                                        <button class="button" id="minusRefNote">-</button>
                                        <button class="button" id="plusRefNote">+</button>
                                    </fieldset>
                                </div>
                            </div>
						</div>
					</div>
				</div>
			</section>
            
            <!-- APIS USED -->
            <section class="section">
				<header class="section-header">
					<h2 class="subheading">APIs used</h2>
				</header>
				<div class="section-body">
                    <section class="subsection">
						<header class="subsection-header">
							<h3 class="title">Web Audio</h3>
						</header>
						<div class="subsection-body">
							<div class="row">
								<div class="col-sm-24">
									<p>This API is the core of this demo. We use it to perform different tasks, from generating synthetic sounds, to analysing the sound we get, to chanelling the sound to whatever the default audio output device is.</p>
                                    
                                    <h4 class="subtitle">AudioContext</h4>
                                    <p>This is the entry point of Web Audio, and responsible of generating all the AudioNode instances we use throughout of this demo.</p>
                                    <p>We start by checking if the browser supports Web Audio by looking at whether window.AudioContext (or its webkit prefixed version) is defined. This also will set <em>window.AudioContext</em> so we can easily instance it later if the browser, in fact, supports it.</p>
                                    <script src="https://gist.github.com/quimbs/564c57dba361d41edcd0.js"></script>
                                    
                                    <h4 class="subtitle">OscillatorNode</h4>
                                    <p>In order to generate a synthetic sound to tune by ear we use an OscillatorNode, which we can configure to play at a specific frequency.
                                    </p>
                                    <p>The "Base frequency" controls that you see when you enable this part of the tuner adjusts the frequency of A4, which is used as a reference for the rest of the notes. Although all of them can be calculated from A4's frequency, we have pre-calculated them and placed them in a <a href="https://gist.github.com/quimbs/fe52bc25dca44cd3fcb8#file-notes-json" target="_blank">notes.json</a> file that we dynamically load at runtime. After that, it's just a matter of iterating through the array of notes for a particular A4 frequency and set the correct frequency in the oscillator node.
                                    </p>
                                    <script src="https://gist.github.com/quimbs/fe52bc25dca44cd3fcb8.js?file=oscillatorNode.js"></script>
                                    
                                    <h4 class="subtitle">MediaStreamAudioSourceNode</h4>
                                    <p>We use this type of node as a source AudioNode with the stream of data we get from the Media Stream API (see below). You should take into account that <strong>the sampling frequency used will match the sampling rate that your output device uses (typically 44.1kHz or 48kHz).</strong></p>
                                    <script src="https://gist.github.com/quimbs/1744eb50a461257af4ad.js"></script>
                                    
                                    <h4 class="subtitle">AnalyserNode</h4>
                                    <p>This node receives data from the MediaStreamAudioSourceNode and performs a Fast Fourier Transform on those samples. This data is later used by an autocorrelation algorithm to detect the pitch of the sound. For this node we set an fftSize of 2048 (the maximum allowed by the Web Audio API), which although is very tight for such a big sampling rate (we can only fit a tiny fraction of a second in that space) it is the best we can do without downsampling the stream by ourselves.</p>
                                    
                                    <script src="https://gist.github.com/quimbs/44f68f011c2c36690f2e.js"></script>
								</div>
							</div>
						</div>
					</section>
                    
					<section class="subsection">
						<header class="subsection-header">
							<h3 class="title">Media Stream</h3>
						</header>
						<div class="subsection-body">
							<div class="row">
                                <div class="col-sm-24">
                                    <p>From this API we only use one specific function to access the audio input device, generally the microphone: <strong>getUserMedia</strong>. Just like with AudioContext, we whould consider the possibility that the API may be prefixed in some browsers or older versions of them.</p>
                                    <script src="https://gist.github.com/quimbs/cedded5dcebba82421e6.js"></script>
                                </div>
							</div>
						</div>
					</section>
					
				</div>
			</section>
            
			<!-- GRID -->
			<section class="section">
				<header class="section-header">
					<h2 class="subheading">Pitch detection</h2>
				</header>
				<div class="section-body">

					<div class="subsection">
						<header class="subsection-header">
							<h3 class="title">Autocorrelation</h3>
						</header>
						<div class="subsection-body">
                            <p>There is a variety of methods to detect the pitch of a sound, some work in the frequency domain (like <em>HPS</em>, or <em>Harmonic Product Spectrum</em>), while some others do in the time domain (like <em>Autocorrelation</em>). With such a high sampling rate, we can only fit a small fraction of a second in the buffer used by the AnalyserNode. In these conditions, the latter algorithm usually does a better job than the former, and this is why we chose it for this demo.</p>
                            
                            <p>Autocorrelation is the process of cross-correlating a signal with a time-delayed version of itself. In other words, we will be comparing a signal at two different points in time. As <a href="http://en.wikipedia.org/wiki/Autocorrelation" target="_blank">Wikipedia</a> puts it:</p>
                            <blockquote cite="http://en.wikipedia.org/wiki/Autocorrelation">
                                It is a mathematical tool for finding repeating patterns, such as the presence of a periodic signal obscured by noise, or identifying the missing fundamental frequency in a signal implied by its harmonic frequencies.
                            </blockquote>
                            
                            <p>Given a delay time \(k\) we:</p>
                                <ol>
                                    <li>Find the value at a time \(t\)</li>
                                    <li>Find the value at a time \(t+k\)</li>
                                    <li>Multiply those values together</li>
                                    <li>Accumulate those products over a series of times (1000 in our code)</li>
                                    <li>Divide by the number of samples to get the average</li>
                                </ol>
                            
                            <p>As seen in <a href="http://www.phy.mtu.edu/~suits/autocorrelation.html" target="_blank">this page</a>, the resulting formula would be something like this:</p>
                            <p>\[ R(k) = \frac{1}{t_{max} - t_{min}} \int_{t_{min}}^{t_{max}}s(t)s(t+k)dt \]</p>
                            
                            <script src="https://gist.github.com/quimbs/9a7bf7ddd161dac727c6.js"></script>
                            
                            <p>In addition, as you can see above, we also normalize the data. Since we are working with an array of bytes (0-255), we subtract 128 and divide by the same value.</p>
                            <p>Remember that we are working with periodic signals. As you may imagine, the highest correlation will happen once that signal "repeats itself", i.e. that \(bestK\) will match the period (in frames) of the fundamental frequency. In order to get that frequency we just need to divide the sampling rate by that distance \(bestK\).</p>
						</div>
					</div>
					<div class="subsection">
						<header class="subsection-header">
							<h3 class="title">Finding the right note</h3>
						</header>
						<div class="subsection-body">
                            <p>Now that we have the fundamental frequency, we just need to find the note with the closest frequency. Since the <em>notesArray</em> that we showed in previous code snippets is already sorted by this value, we only need to perform a binary search to find it.</p>
                            <script src="https://gist.github.com/quimbs/61914ff8bbc0fd72eaf4.js"></script>
				        </div>
                    </div>
                    
                    <div class="subsection">
						<header class="subsection-header">
							<h3 class="title">Calculating the cents off pitch</h3>
						</header>
						<div class="subsection-body">
                            <p>The last step, given the fundalmental frequency that we have found and the frequency of the closest note, we need how far the former one is from the latter. This is done using the following formula.</p>
                            <p>\[ cents = \left \lfloor 1200 \frac{ \log_{10}(f/refF) }{\log_{10}2} \right \rfloor \]</p>
                            <p>Where \(f\) is the fundamental frequency and \(refF\) is the frequency from the closes note. Since \(\log_{10}2\) is a constant that we can precalculate, we just use it directly in our code.</p>
                            <script src="https://gist.github.com/quimbs/e91f0ce6997514c967a3.js"></script>
				        </div>
                    </div>
                </div>
			</section>
		</div>
	</div>
</article>

<script type="text/javascript" src="https://code.jquery.com/jquery-2.1.3.min.js"></script>
<script type="text/javascript" src="scripts/demo.js"></script>
<script type="text/javascript" src="http://bernii.github.io/gauge.js/dist/gauge.min.js"></script>
<script type="text/javascript"
  src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

</body>
</html>